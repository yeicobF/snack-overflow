{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e11af4e9-0b8e-4d97-9675-84be2d719671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3c7a625d-36c7-49f7-a603-8064bcafb84d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22647724139043623, 0.4496248294374869, 0.5503751705625131,\n",
       "        0.40557610852958464, 0.3478720327973481, 1.0],\n",
       "       [0.18618179118342765, 0.4821578301371476, 0.5178421698628524,\n",
       "        0.6956426140190686, 0.5875519736615444, 1.0],\n",
       "       [0.06715071025369221, 0.41822527804611914, 0.5817747219538809,\n",
       "        0.034839608532212456, 0.050847197345244725, 1.0],\n",
       "       [0.17786688472486434, 0.519126803080574, 0.480873196919426,\n",
       "        0.4193320081082101, 0.37439247314872637, 1.0],\n",
       "       [0.09547184045751664, 0.47709694014604537, 0.5229030598539546,\n",
       "        0.7171062934084195, 0.5525837418705247, 1.0]], dtype=object)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset\n",
    "ds = np.load(\"dataset.npy\", allow_pickle=True)\n",
    "ds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c90aab23-41d0-4c86-a3aa-5f24293cbad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Sequential model with 3 layers\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(4, activation=\"relu\", name=\"h_layer\"),\n",
    "        layers.Dense(1, name=\"output_layer\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#Shuffle\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(ds)\n",
    "\n",
    "#Get the training, validation and test splits\n",
    "train_bound = int(.8*len(ds))\n",
    "validation_bound = int(train_bound + .1 * len(ds))\n",
    "\n",
    "x_train, y_train = x = np.asarray(ds[:train_bound][:,:5]).astype('float32'), np.asarray(ds[:train_bound][:,-1]).astype('float32')\n",
    "x_val, y_val = np.asarray(ds[train_bound:validation_bound][:,:5]).astype('float32'), np.asarray(ds[train_bound:validation_bound][:,-1]).astype('float32')\n",
    "x_test, y_test = np.asarray(ds[validation_bound:][:,:5]).astype('float32'), np.asarray(ds[validation_bound:][:,-1]).astype('float32')\n",
    "\n",
    "# compile\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.RMSprop(),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.Accuracy()],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3c3c8b71-a751-4bcb-b109-b8022bab17fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "88/88 [==============================] - 0s 713us/step - loss: 4.3580 - accuracy: 0.0000e+00 - val_loss: 3.9607 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "88/88 [==============================] - 0s 370us/step - loss: 3.5039 - accuracy: 0.0000e+00 - val_loss: 2.8654 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/100\n",
      "88/88 [==============================] - 0s 365us/step - loss: 1.9027 - accuracy: 0.0000e+00 - val_loss: 1.1306 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/100\n",
      "88/88 [==============================] - 0s 350us/step - loss: 0.8926 - accuracy: 0.0000e+00 - val_loss: 0.7562 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/100\n",
      "88/88 [==============================] - 0s 370us/step - loss: 0.7293 - accuracy: 0.0000e+00 - val_loss: 0.7106 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/100\n",
      "88/88 [==============================] - 0s 347us/step - loss: 0.7049 - accuracy: 0.0000e+00 - val_loss: 0.6998 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/100\n",
      "88/88 [==============================] - 0s 355us/step - loss: 0.6969 - accuracy: 0.0000e+00 - val_loss: 0.6944 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/100\n",
      "88/88 [==============================] - 0s 356us/step - loss: 0.6912 - accuracy: 0.0000e+00 - val_loss: 0.6899 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/100\n",
      "88/88 [==============================] - 0s 361us/step - loss: 0.6862 - accuracy: 0.0000e+00 - val_loss: 0.6856 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/100\n",
      "88/88 [==============================] - 0s 348us/step - loss: 0.6813 - accuracy: 0.0000e+00 - val_loss: 0.6811 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/100\n",
      "88/88 [==============================] - 0s 344us/step - loss: 0.6761 - accuracy: 0.0000e+00 - val_loss: 0.6764 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/100\n",
      "88/88 [==============================] - 0s 348us/step - loss: 0.6698 - accuracy: 0.0000e+00 - val_loss: 0.6703 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/100\n",
      "88/88 [==============================] - 0s 363us/step - loss: 0.6618 - accuracy: 0.0000e+00 - val_loss: 0.6627 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/100\n",
      "88/88 [==============================] - 0s 350us/step - loss: 0.6518 - accuracy: 0.0000e+00 - val_loss: 0.6532 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/100\n",
      "88/88 [==============================] - 0s 352us/step - loss: 0.6387 - accuracy: 0.0000e+00 - val_loss: 0.6411 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/100\n",
      "88/88 [==============================] - 0s 355us/step - loss: 0.6225 - accuracy: 0.0000e+00 - val_loss: 0.6254 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/100\n",
      "88/88 [==============================] - 0s 337us/step - loss: 0.6025 - accuracy: 0.0000e+00 - val_loss: 0.6078 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/100\n",
      "88/88 [==============================] - 0s 355us/step - loss: 0.5812 - accuracy: 0.0000e+00 - val_loss: 0.5892 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/100\n",
      "88/88 [==============================] - 0s 342us/step - loss: 0.5593 - accuracy: 0.0000e+00 - val_loss: 0.5701 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/100\n",
      "88/88 [==============================] - 0s 344us/step - loss: 0.5364 - accuracy: 0.0000e+00 - val_loss: 0.5492 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/100\n",
      "88/88 [==============================] - 0s 345us/step - loss: 0.5144 - accuracy: 0.0000e+00 - val_loss: 0.5299 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/100\n",
      "88/88 [==============================] - 0s 357us/step - loss: 0.4925 - accuracy: 0.0000e+00 - val_loss: 0.5108 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/100\n",
      "88/88 [==============================] - 0s 349us/step - loss: 0.4727 - accuracy: 0.0000e+00 - val_loss: 0.4936 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/100\n",
      "88/88 [==============================] - 0s 346us/step - loss: 0.4530 - accuracy: 0.0000e+00 - val_loss: 0.4753 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/100\n",
      "88/88 [==============================] - 0s 353us/step - loss: 0.4323 - accuracy: 0.0000e+00 - val_loss: 0.4565 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/100\n",
      "88/88 [==============================] - 0s 354us/step - loss: 0.4140 - accuracy: 0.0000e+00 - val_loss: 0.4432 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/100\n",
      "88/88 [==============================] - 0s 346us/step - loss: 0.3998 - accuracy: 0.0000e+00 - val_loss: 0.4287 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/100\n",
      "88/88 [==============================] - 0s 340us/step - loss: 0.3869 - accuracy: 0.0000e+00 - val_loss: 0.4172 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/100\n",
      "88/88 [==============================] - 0s 340us/step - loss: 0.3758 - accuracy: 0.0000e+00 - val_loss: 0.4062 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/100\n",
      "88/88 [==============================] - 0s 347us/step - loss: 0.3636 - accuracy: 0.0000e+00 - val_loss: 0.3951 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/100\n",
      "88/88 [==============================] - 0s 349us/step - loss: 0.3521 - accuracy: 0.0000e+00 - val_loss: 0.3827 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/100\n",
      "88/88 [==============================] - 0s 349us/step - loss: 0.3408 - accuracy: 0.0000e+00 - val_loss: 0.3707 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/100\n",
      "88/88 [==============================] - 0s 348us/step - loss: 0.3291 - accuracy: 0.0000e+00 - val_loss: 0.3583 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/100\n",
      "88/88 [==============================] - 0s 349us/step - loss: 0.3183 - accuracy: 0.0000e+00 - val_loss: 0.3476 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/100\n",
      "88/88 [==============================] - 0s 349us/step - loss: 0.3084 - accuracy: 0.0000e+00 - val_loss: 0.3371 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/100\n",
      "88/88 [==============================] - 0s 348us/step - loss: 0.2987 - accuracy: 0.0000e+00 - val_loss: 0.3269 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/100\n",
      "88/88 [==============================] - 0s 354us/step - loss: 0.2894 - accuracy: 0.0000e+00 - val_loss: 0.3169 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/100\n",
      "88/88 [==============================] - 0s 348us/step - loss: 0.2810 - accuracy: 0.0000e+00 - val_loss: 0.3086 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/100\n",
      "88/88 [==============================] - 0s 349us/step - loss: 0.2736 - accuracy: 0.0000e+00 - val_loss: 0.3001 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/100\n",
      "88/88 [==============================] - 0s 336us/step - loss: 0.2661 - accuracy: 0.0000e+00 - val_loss: 0.2926 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/100\n",
      "88/88 [==============================] - 0s 353us/step - loss: 0.2600 - accuracy: 0.0000e+00 - val_loss: 0.2857 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/100\n",
      "88/88 [==============================] - 0s 349us/step - loss: 0.2547 - accuracy: 0.0000e+00 - val_loss: 0.2801 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/100\n",
      "88/88 [==============================] - 0s 355us/step - loss: 0.2497 - accuracy: 0.0000e+00 - val_loss: 0.2772 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/100\n",
      "88/88 [==============================] - 0s 349us/step - loss: 0.2454 - accuracy: 0.0000e+00 - val_loss: 0.2700 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/100\n",
      "88/88 [==============================] - 0s 349us/step - loss: 0.2433 - accuracy: 0.0000e+00 - val_loss: 0.2655 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/100\n",
      "88/88 [==============================] - 0s 361us/step - loss: 0.2395 - accuracy: 0.0000e+00 - val_loss: 0.2619 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/100\n",
      "88/88 [==============================] - 0s 350us/step - loss: 0.2363 - accuracy: 0.0000e+00 - val_loss: 0.2593 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/100\n",
      "88/88 [==============================] - 0s 352us/step - loss: 0.2338 - accuracy: 0.0000e+00 - val_loss: 0.2557 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/100\n",
      "88/88 [==============================] - 0s 354us/step - loss: 0.2315 - accuracy: 0.0000e+00 - val_loss: 0.2531 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/100\n",
      "88/88 [==============================] - 0s 351us/step - loss: 0.2298 - accuracy: 0.0000e+00 - val_loss: 0.2514 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/100\n",
      "88/88 [==============================] - 0s 353us/step - loss: 0.2282 - accuracy: 0.0000e+00 - val_loss: 0.2492 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/100\n",
      "88/88 [==============================] - 0s 357us/step - loss: 0.2270 - accuracy: 0.0000e+00 - val_loss: 0.2481 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/100\n",
      "88/88 [==============================] - 0s 353us/step - loss: 0.2262 - accuracy: 0.0000e+00 - val_loss: 0.2476 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/100\n",
      "88/88 [==============================] - 0s 351us/step - loss: 0.2288 - accuracy: 0.0000e+00 - val_loss: 0.2456 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/100\n",
      "88/88 [==============================] - 0s 344us/step - loss: 0.2296 - accuracy: 0.0000e+00 - val_loss: 0.2438 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/100\n",
      "88/88 [==============================] - 0s 347us/step - loss: 0.2275 - accuracy: 0.0000e+00 - val_loss: 0.2436 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/100\n",
      "88/88 [==============================] - 0s 343us/step - loss: 0.2248 - accuracy: 0.0000e+00 - val_loss: 0.2439 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/100\n",
      "88/88 [==============================] - 0s 347us/step - loss: 0.2267 - accuracy: 0.0000e+00 - val_loss: 0.2426 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/100\n",
      "88/88 [==============================] - 0s 346us/step - loss: 0.2243 - accuracy: 0.0000e+00 - val_loss: 0.2416 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/100\n",
      "88/88 [==============================] - 0s 343us/step - loss: 0.2274 - accuracy: 0.0000e+00 - val_loss: 0.2440 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/100\n",
      "88/88 [==============================] - 0s 343us/step - loss: 0.2285 - accuracy: 0.0000e+00 - val_loss: 0.2407 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/100\n",
      "88/88 [==============================] - 0s 339us/step - loss: 0.2297 - accuracy: 0.0000e+00 - val_loss: 0.2440 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/100\n",
      "88/88 [==============================] - 0s 339us/step - loss: 0.2288 - accuracy: 0.0000e+00 - val_loss: 0.2558 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/100\n",
      "88/88 [==============================] - 0s 346us/step - loss: 0.2289 - accuracy: 0.0000e+00 - val_loss: 0.2390 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/100\n",
      "88/88 [==============================] - 0s 341us/step - loss: 0.2300 - accuracy: 0.0000e+00 - val_loss: 0.2387 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/100\n",
      "88/88 [==============================] - 0s 347us/step - loss: 0.2315 - accuracy: 0.0000e+00 - val_loss: 0.2388 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/100\n",
      "88/88 [==============================] - 0s 347us/step - loss: 0.2284 - accuracy: 0.0000e+00 - val_loss: 0.2539 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/100\n",
      "88/88 [==============================] - 0s 343us/step - loss: 0.2291 - accuracy: 0.0000e+00 - val_loss: 0.2537 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/100\n",
      "88/88 [==============================] - 0s 346us/step - loss: 0.2285 - accuracy: 0.0000e+00 - val_loss: 0.2388 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/100\n",
      "88/88 [==============================] - 0s 340us/step - loss: 0.2273 - accuracy: 0.0000e+00 - val_loss: 0.2405 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/100\n",
      "88/88 [==============================] - 0s 344us/step - loss: 0.2308 - accuracy: 0.0000e+00 - val_loss: 0.2537 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/100\n",
      "88/88 [==============================] - 0s 350us/step - loss: 0.2249 - accuracy: 0.0000e+00 - val_loss: 0.2382 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/100\n",
      "88/88 [==============================] - 0s 351us/step - loss: 0.2273 - accuracy: 0.0000e+00 - val_loss: 0.2527 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/100\n",
      "88/88 [==============================] - 0s 349us/step - loss: 0.2302 - accuracy: 0.0000e+00 - val_loss: 0.2526 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/100\n",
      "88/88 [==============================] - 0s 343us/step - loss: 0.2313 - accuracy: 0.0000e+00 - val_loss: 0.2530 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/100\n",
      "88/88 [==============================] - 0s 341us/step - loss: 0.2284 - accuracy: 0.0000e+00 - val_loss: 0.2522 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/100\n",
      "88/88 [==============================] - 0s 348us/step - loss: 0.2293 - accuracy: 0.0000e+00 - val_loss: 0.2522 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/100\n",
      "88/88 [==============================] - 0s 343us/step - loss: 0.2299 - accuracy: 0.0000e+00 - val_loss: 0.2523 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/100\n",
      "88/88 [==============================] - 0s 347us/step - loss: 0.2289 - accuracy: 0.0000e+00 - val_loss: 0.2394 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/100\n",
      "88/88 [==============================] - 0s 354us/step - loss: 0.2275 - accuracy: 0.0000e+00 - val_loss: 0.2516 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/100\n",
      "88/88 [==============================] - 0s 350us/step - loss: 0.2282 - accuracy: 0.0000e+00 - val_loss: 0.2516 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/100\n",
      "88/88 [==============================] - 0s 349us/step - loss: 0.2306 - accuracy: 0.0000e+00 - val_loss: 0.2508 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/100\n",
      "88/88 [==============================] - 0s 354us/step - loss: 0.2309 - accuracy: 0.0000e+00 - val_loss: 0.2511 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/100\n",
      "88/88 [==============================] - 0s 346us/step - loss: 0.2306 - accuracy: 0.0000e+00 - val_loss: 0.2517 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/100\n",
      "88/88 [==============================] - 0s 360us/step - loss: 0.2311 - accuracy: 0.0000e+00 - val_loss: 0.2512 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/100\n",
      "88/88 [==============================] - 0s 356us/step - loss: 0.2316 - accuracy: 0.0000e+00 - val_loss: 0.2509 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/100\n",
      "88/88 [==============================] - 0s 345us/step - loss: 0.2318 - accuracy: 0.0000e+00 - val_loss: 0.2511 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/100\n",
      "88/88 [==============================] - 0s 352us/step - loss: 0.2323 - accuracy: 0.0000e+00 - val_loss: 0.2509 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/100\n",
      "88/88 [==============================] - 0s 352us/step - loss: 0.2332 - accuracy: 0.0000e+00 - val_loss: 0.2513 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/100\n",
      "88/88 [==============================] - 0s 354us/step - loss: 0.2303 - accuracy: 0.0000e+00 - val_loss: 0.2513 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/100\n",
      "88/88 [==============================] - 0s 352us/step - loss: 0.2327 - accuracy: 0.0000e+00 - val_loss: 0.2508 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/100\n",
      "88/88 [==============================] - 0s 352us/step - loss: 0.2338 - accuracy: 0.0000e+00 - val_loss: 0.2748 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/100\n",
      "88/88 [==============================] - 0s 351us/step - loss: 0.2377 - accuracy: 0.0000e+00 - val_loss: 0.2504 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/100\n",
      "88/88 [==============================] - 0s 345us/step - loss: 0.2319 - accuracy: 0.0000e+00 - val_loss: 0.2757 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/100\n",
      "88/88 [==============================] - 0s 346us/step - loss: 0.2326 - accuracy: 0.0000e+00 - val_loss: 0.2509 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/100\n",
      "88/88 [==============================] - 0s 344us/step - loss: 0.2323 - accuracy: 0.0000e+00 - val_loss: 0.2513 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/100\n",
      "88/88 [==============================] - 0s 346us/step - loss: 0.2314 - accuracy: 0.0000e+00 - val_loss: 0.2516 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/100\n",
      "88/88 [==============================] - 0s 346us/step - loss: 0.2321 - accuracy: 0.0000e+00 - val_loss: 0.2512 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/100\n",
      "88/88 [==============================] - 0s 373us/step - loss: 0.2363 - accuracy: 0.0000e+00 - val_loss: 0.2514 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/100\n",
      "88/88 [==============================] - 0s 348us/step - loss: 0.2325 - accuracy: 0.0000e+00 - val_loss: 0.2510 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    epochs=100, \n",
    "    validation_data=(x_val, y_val),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "497338e6-b1ef-412f-82f6-76e787f8e25d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.3976382315158844,\n",
       "  0.37734681367874146,\n",
       "  0.3573972284793854,\n",
       "  0.3367692232131958,\n",
       "  0.31686198711395264,\n",
       "  0.2974555790424347,\n",
       "  0.2781427800655365,\n",
       "  0.26132383942604065,\n",
       "  0.2457609921693802,\n",
       "  0.2308734506368637,\n",
       "  0.21741710603237152,\n",
       "  0.204332172870636,\n",
       "  0.19311970472335815,\n",
       "  0.18293873965740204,\n",
       "  0.17443867027759552,\n",
       "  0.16735221445560455,\n",
       "  0.16058799624443054,\n",
       "  0.15494860708713531,\n",
       "  0.14947204291820526,\n",
       "  0.14463773369789124,\n",
       "  0.1401238888502121,\n",
       "  0.13675102591514587,\n",
       "  0.1331622451543808,\n",
       "  0.12987962365150452,\n",
       "  0.12655045092105865,\n",
       "  0.1240612119436264,\n",
       "  0.1211976706981659,\n",
       "  0.11911717802286148,\n",
       "  0.11726635694503784,\n",
       "  0.1149597093462944,\n",
       "  0.11286794394254684,\n",
       "  0.11143162101507187,\n",
       "  0.10981406271457672,\n",
       "  0.10821133106946945,\n",
       "  0.10712946951389313,\n",
       "  0.10783205181360245,\n",
       "  0.10460561513900757,\n",
       "  0.10347387939691544,\n",
       "  0.10233740508556366,\n",
       "  0.10160195827484131,\n",
       "  0.10045050829648972,\n",
       "  0.09977392852306366,\n",
       "  0.09882653504610062,\n",
       "  0.09826436638832092,\n",
       "  0.09736049175262451,\n",
       "  0.09664521366357803,\n",
       "  0.09599175304174423,\n",
       "  0.09540033340454102,\n",
       "  0.09471480548381805,\n",
       "  0.09547789394855499,\n",
       "  0.09352272003889084,\n",
       "  0.09265746921300888,\n",
       "  0.09227783977985382,\n",
       "  0.09139785170555115,\n",
       "  0.09133020043373108,\n",
       "  0.09081844240427017,\n",
       "  0.09042403101921082,\n",
       "  0.08997940272092819,\n",
       "  0.08962198346853256,\n",
       "  0.08942044526338577,\n",
       "  0.08925998210906982,\n",
       "  0.08856420964002609,\n",
       "  0.09010486304759979,\n",
       "  0.08994854241609573,\n",
       "  0.08749185502529144,\n",
       "  0.08886463195085526,\n",
       "  0.0871226117014885,\n",
       "  0.08843384683132172,\n",
       "  0.08801402151584625,\n",
       "  0.08808111399412155,\n",
       "  0.08703477680683136,\n",
       "  0.08589020371437073,\n",
       "  0.08725783973932266,\n",
       "  0.08443814516067505,\n",
       "  0.08421313762664795,\n",
       "  0.08578644692897797,\n",
       "  0.08583307266235352,\n",
       "  0.08619463443756104,\n",
       "  0.0830247774720192,\n",
       "  0.08972688764333725,\n",
       "  0.08283309638500214,\n",
       "  0.08395892381668091,\n",
       "  0.08266961574554443,\n",
       "  0.08858218789100647,\n",
       "  0.08420377224683762,\n",
       "  0.08367160707712173,\n",
       "  0.08582794666290283,\n",
       "  0.08355336636304855,\n",
       "  0.08333946019411087,\n",
       "  0.0849943459033966,\n",
       "  0.08297421783208847,\n",
       "  0.08246250450611115,\n",
       "  0.08469674736261368,\n",
       "  0.08259778469800949,\n",
       "  0.08429449796676636,\n",
       "  0.08224111795425415,\n",
       "  0.08411913365125656,\n",
       "  0.0833263024687767,\n",
       "  0.08552178740501404,\n",
       "  0.08128371834754944],\n",
       " 'binary_crossentropy': [0.3976382315158844,\n",
       "  0.37734681367874146,\n",
       "  0.3573972284793854,\n",
       "  0.3367692232131958,\n",
       "  0.31686198711395264,\n",
       "  0.2974555790424347,\n",
       "  0.2781427800655365,\n",
       "  0.26132383942604065,\n",
       "  0.2457609921693802,\n",
       "  0.2308734506368637,\n",
       "  0.21741710603237152,\n",
       "  0.204332172870636,\n",
       "  0.19311970472335815,\n",
       "  0.18293873965740204,\n",
       "  0.17443867027759552,\n",
       "  0.16735221445560455,\n",
       "  0.16058799624443054,\n",
       "  0.15494860708713531,\n",
       "  0.14947204291820526,\n",
       "  0.14463773369789124,\n",
       "  0.1401238888502121,\n",
       "  0.13675102591514587,\n",
       "  0.1331622451543808,\n",
       "  0.12987962365150452,\n",
       "  0.12655045092105865,\n",
       "  0.1240612119436264,\n",
       "  0.1211976706981659,\n",
       "  0.11911717802286148,\n",
       "  0.11726635694503784,\n",
       "  0.1149597093462944,\n",
       "  0.11286794394254684,\n",
       "  0.11143162101507187,\n",
       "  0.10981406271457672,\n",
       "  0.10821133106946945,\n",
       "  0.10712946951389313,\n",
       "  0.10783205181360245,\n",
       "  0.10460561513900757,\n",
       "  0.10347387939691544,\n",
       "  0.10233740508556366,\n",
       "  0.10160195827484131,\n",
       "  0.10045050829648972,\n",
       "  0.09977392852306366,\n",
       "  0.09882653504610062,\n",
       "  0.09826436638832092,\n",
       "  0.09736049175262451,\n",
       "  0.09664521366357803,\n",
       "  0.09599175304174423,\n",
       "  0.09540033340454102,\n",
       "  0.09471480548381805,\n",
       "  0.09547789394855499,\n",
       "  0.09352272003889084,\n",
       "  0.09265746921300888,\n",
       "  0.09227783977985382,\n",
       "  0.09139785170555115,\n",
       "  0.09133020043373108,\n",
       "  0.09081844240427017,\n",
       "  0.09042403101921082,\n",
       "  0.08997940272092819,\n",
       "  0.08962198346853256,\n",
       "  0.08942044526338577,\n",
       "  0.08925998210906982,\n",
       "  0.08856420964002609,\n",
       "  0.09010486304759979,\n",
       "  0.08994854241609573,\n",
       "  0.08749185502529144,\n",
       "  0.08886463195085526,\n",
       "  0.0871226117014885,\n",
       "  0.08843384683132172,\n",
       "  0.08801402151584625,\n",
       "  0.08808111399412155,\n",
       "  0.08703477680683136,\n",
       "  0.08589020371437073,\n",
       "  0.08725783973932266,\n",
       "  0.08443814516067505,\n",
       "  0.08421313762664795,\n",
       "  0.08578644692897797,\n",
       "  0.08583307266235352,\n",
       "  0.08619463443756104,\n",
       "  0.0830247774720192,\n",
       "  0.08972688764333725,\n",
       "  0.08283309638500214,\n",
       "  0.08395892381668091,\n",
       "  0.08266961574554443,\n",
       "  0.08858218789100647,\n",
       "  0.08420377224683762,\n",
       "  0.08367160707712173,\n",
       "  0.08582794666290283,\n",
       "  0.08355336636304855,\n",
       "  0.08333946019411087,\n",
       "  0.0849943459033966,\n",
       "  0.08297421783208847,\n",
       "  0.08246250450611115,\n",
       "  0.08469674736261368,\n",
       "  0.08259778469800949,\n",
       "  0.08429449796676636,\n",
       "  0.08224111795425415,\n",
       "  0.08411913365125656,\n",
       "  0.0833263024687767,\n",
       "  0.08552178740501404,\n",
       "  0.08128371834754944],\n",
       " 'val_loss': [0.381045937538147,\n",
       "  0.36086827516555786,\n",
       "  0.3415513336658478,\n",
       "  0.31967300176620483,\n",
       "  0.2994924783706665,\n",
       "  0.2802044749259949,\n",
       "  0.2621449828147888,\n",
       "  0.24743877351284027,\n",
       "  0.23163670301437378,\n",
       "  0.21880340576171875,\n",
       "  0.20530906319618225,\n",
       "  0.19384723901748657,\n",
       "  0.18396347761154175,\n",
       "  0.17627385258674622,\n",
       "  0.16731132566928864,\n",
       "  0.16148313879966736,\n",
       "  0.15541717410087585,\n",
       "  0.14974237978458405,\n",
       "  0.1447681337594986,\n",
       "  0.14159223437309265,\n",
       "  0.14328016340732574,\n",
       "  0.13166071474552155,\n",
       "  0.12884941697120667,\n",
       "  0.12560172379016876,\n",
       "  0.12304465472698212,\n",
       "  0.12024613469839096,\n",
       "  0.11822335422039032,\n",
       "  0.11498081684112549,\n",
       "  0.11293169111013412,\n",
       "  0.11218681186437607,\n",
       "  0.10898839682340622,\n",
       "  0.10734523087739944,\n",
       "  0.1072678193449974,\n",
       "  0.10537336021661758,\n",
       "  0.10650390386581421,\n",
       "  0.10345987230539322,\n",
       "  0.10149234533309937,\n",
       "  0.10052982717752457,\n",
       "  0.09975229203701019,\n",
       "  0.09813311696052551,\n",
       "  0.09975702315568924,\n",
       "  0.09604939818382263,\n",
       "  0.09600463509559631,\n",
       "  0.0944037064909935,\n",
       "  0.09744330495595932,\n",
       "  0.09322652220726013,\n",
       "  0.09338784217834473,\n",
       "  0.09306669235229492,\n",
       "  0.09076987206935883,\n",
       "  0.0937388464808464,\n",
       "  0.09081700444221497,\n",
       "  0.0891486331820488,\n",
       "  0.0886479988694191,\n",
       "  0.08866837620735168,\n",
       "  0.08794689923524857,\n",
       "  0.08699365705251694,\n",
       "  0.08752027153968811,\n",
       "  0.08701654523611069,\n",
       "  0.08658069372177124,\n",
       "  0.08677411824464798,\n",
       "  0.08553816378116608,\n",
       "  0.0846392810344696,\n",
       "  0.0845520868897438,\n",
       "  0.08396565914154053,\n",
       "  0.08359917998313904,\n",
       "  0.08362644910812378,\n",
       "  0.08259136974811554,\n",
       "  0.08355232328176498,\n",
       "  0.08253265917301178,\n",
       "  0.08208083361387253,\n",
       "  0.08156003057956696,\n",
       "  0.08260852098464966,\n",
       "  0.08086459338665009,\n",
       "  0.0813741609454155,\n",
       "  0.08019454032182693,\n",
       "  0.0798836499452591,\n",
       "  0.08031259477138519,\n",
       "  0.08002099394798279,\n",
       "  0.08064703643321991,\n",
       "  0.07931309193372726,\n",
       "  0.07966805249452591,\n",
       "  0.0800442323088646,\n",
       "  0.07947041839361191,\n",
       "  0.07779064774513245,\n",
       "  0.07820992916822433,\n",
       "  0.07797329127788544,\n",
       "  0.07756118476390839,\n",
       "  0.07783303409814835,\n",
       "  0.07717576622962952,\n",
       "  0.07790614664554596,\n",
       "  0.0775030329823494,\n",
       "  0.07802696526050568,\n",
       "  0.07671034336090088,\n",
       "  0.07586953043937683,\n",
       "  0.07601245492696762,\n",
       "  0.07581554353237152,\n",
       "  0.07674497365951538,\n",
       "  0.0761379823088646,\n",
       "  0.08035747706890106,\n",
       "  0.07823007553815842],\n",
       " 'val_binary_crossentropy': [0.381045937538147,\n",
       "  0.36086827516555786,\n",
       "  0.3415513336658478,\n",
       "  0.31967300176620483,\n",
       "  0.2994924783706665,\n",
       "  0.2802044749259949,\n",
       "  0.2621449828147888,\n",
       "  0.24743877351284027,\n",
       "  0.23163670301437378,\n",
       "  0.21880340576171875,\n",
       "  0.20530906319618225,\n",
       "  0.19384723901748657,\n",
       "  0.18396347761154175,\n",
       "  0.17627385258674622,\n",
       "  0.16731132566928864,\n",
       "  0.16148313879966736,\n",
       "  0.15541717410087585,\n",
       "  0.14974237978458405,\n",
       "  0.1447681337594986,\n",
       "  0.14159223437309265,\n",
       "  0.14328016340732574,\n",
       "  0.13166071474552155,\n",
       "  0.12884941697120667,\n",
       "  0.12560172379016876,\n",
       "  0.12304465472698212,\n",
       "  0.12024613469839096,\n",
       "  0.11822335422039032,\n",
       "  0.11498081684112549,\n",
       "  0.11293169111013412,\n",
       "  0.11218681186437607,\n",
       "  0.10898839682340622,\n",
       "  0.10734523087739944,\n",
       "  0.1072678193449974,\n",
       "  0.10537336021661758,\n",
       "  0.10650390386581421,\n",
       "  0.10345987230539322,\n",
       "  0.10149234533309937,\n",
       "  0.10052982717752457,\n",
       "  0.09975229203701019,\n",
       "  0.09813311696052551,\n",
       "  0.09975702315568924,\n",
       "  0.09604939818382263,\n",
       "  0.09600463509559631,\n",
       "  0.0944037064909935,\n",
       "  0.09744330495595932,\n",
       "  0.09322652220726013,\n",
       "  0.09338784217834473,\n",
       "  0.09306669235229492,\n",
       "  0.09076987206935883,\n",
       "  0.0937388464808464,\n",
       "  0.09081700444221497,\n",
       "  0.0891486331820488,\n",
       "  0.0886479988694191,\n",
       "  0.08866837620735168,\n",
       "  0.08794689923524857,\n",
       "  0.08699365705251694,\n",
       "  0.08752027153968811,\n",
       "  0.08701654523611069,\n",
       "  0.08658069372177124,\n",
       "  0.08677411824464798,\n",
       "  0.08553816378116608,\n",
       "  0.0846392810344696,\n",
       "  0.0845520868897438,\n",
       "  0.08396565914154053,\n",
       "  0.08359917998313904,\n",
       "  0.08362644910812378,\n",
       "  0.08259136974811554,\n",
       "  0.08355232328176498,\n",
       "  0.08253265917301178,\n",
       "  0.08208083361387253,\n",
       "  0.08156003057956696,\n",
       "  0.08260852098464966,\n",
       "  0.08086459338665009,\n",
       "  0.0813741609454155,\n",
       "  0.08019454032182693,\n",
       "  0.0798836499452591,\n",
       "  0.08031259477138519,\n",
       "  0.08002099394798279,\n",
       "  0.08064703643321991,\n",
       "  0.07931309193372726,\n",
       "  0.07966805249452591,\n",
       "  0.0800442323088646,\n",
       "  0.07947041839361191,\n",
       "  0.07779064774513245,\n",
       "  0.07820992916822433,\n",
       "  0.07797329127788544,\n",
       "  0.07756118476390839,\n",
       "  0.07783303409814835,\n",
       "  0.07717576622962952,\n",
       "  0.07790614664554596,\n",
       "  0.0775030329823494,\n",
       "  0.07802696526050568,\n",
       "  0.07671034336090088,\n",
       "  0.07586953043937683,\n",
       "  0.07601245492696762,\n",
       "  0.07581554353237152,\n",
       "  0.07674497365951538,\n",
       "  0.0761379823088646,\n",
       "  0.08035747706890106,\n",
       "  0.07823007553815842]}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d639bcef-d990-4356-b03f-a2b40f657cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 692us/step - loss: 0.0771 - binary_crossentropy: 0.0771\n",
      "test loss, test acc: [0.07708363980054855, 0.07708363980054855]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(x_test, y_test, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9e13724b-5726-43cd-b0c7-9d639cb14f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('our_model.keras')  # The file needs to end with the .keras extension"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "angel_hack",
   "language": "python",
   "name": "angel_hack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
